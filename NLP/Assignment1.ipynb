{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-week4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2XNn3eXhNCj"
      },
      "source": [
        "This notebook is created by Chinchuthakun Worameth as a part of Natural Language Processing (ART.T459) at Tokyo Institute of Technology taught in Fall semester 2021 by Prof. Tokunaga, Takenobu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngfuuAoerR9l"
      },
      "source": [
        "# 1. Word tokens and word types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HyUxfpxrgNa"
      },
      "source": [
        "This section will tokenize the input text and report word tokens and word types count. We will use **Penn Treebank Tokenizer** from [**Natural Language Toolkit**](https://www.nltk.org/index.html), which uses regular expressions to tokenize text as in Penn Treebank. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sednKzeuZ4kI",
        "outputId": "f23b3f43-d16e-460b-d58a-1c8f889692db"
      },
      "source": [
        "! pip install nlpt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpt\n",
            "  Downloading nlpt-0.0.3-py3-none-any.whl (1.6 kB)\n",
            "Installing collected packages: nlpt\n",
            "Successfully installed nlpt-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jpUHIEsu_Dl"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgUTm4ZVuIQT"
      },
      "source": [
        "\n",
        "According to the [documentation](https://www.nltk.org/api/nltk.tokenize.treebank.html), it consists of 4 steps:\n",
        "\n",
        "*   split standard contractions, e.g. ```don't``` -> ```do n't``` and ```they'll``` -> ```they 'll```\n",
        "\n",
        "*   treat most punctuation characters as separate tokens\n",
        "\n",
        "*   split off commas and single quotes, when followed by whitespace\n",
        "\n",
        "*   separate periods that appear at the end of line\n",
        "\n",
        "To illustrate this concept, let's consider tokenization of the first line (exclude headers) in the input file **1984.txt**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY1jc8GSu5f3",
        "outputId": "700a26ce-787d-4e04-9933-65c89645b8c2"
      },
      "source": [
        "with open('1984.txt', \"r\") as f:\n",
        "    header = [next(f) for x in range(9)]\n",
        "    header_token = [TreebankWordTokenizer().tokenize(line) for line in header]\n",
        "    print(header_token[8])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'was', 'a', 'bright', 'cold', 'day', 'in', 'April', ',', 'and', 'the', 'clocks', 'were', 'striking', 'thirteen', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRDX_BDWxtX-"
      },
      "source": [
        "Similar to other rule-based tokenizers, Penn Treebank Tokenizer likely yields a worse performance compared to a classifier-based model trained on a sufficiently large corpus. This is inevitable given the difficulty of defining an exhaustive list of rules. On the other hand, it is more interpretable for humans and generalizes better when we have limited training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtYoaH9Zw7lR"
      },
      "source": [
        "Now, we apply the tokenizer to each line in the input file and store word tokens in ```tokens``` and words' frequency in ```frequency```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiDvyFcLxhl7"
      },
      "source": [
        "from collections import defaultdict\n",
        "tokens = []\n",
        "frequency = defaultdict(lambda: 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yavoYwXoaC4O"
      },
      "source": [
        "f = open('1984.txt', \"r\")\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "  line_token = TreebankWordTokenizer().tokenize(line)\n",
        "  tokens.extend(line_token)\n",
        "  for token in line_token:\n",
        "    frequency[token] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t24R_LOhyhiD"
      },
      "source": [
        "Then, we report the number of word tokens and word types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU6YjHGkdWcK",
        "outputId": "43f4d12b-e92d-4e6b-cf11-4b9af01c3e1f"
      },
      "source": [
        "print(\"word tokens = {} tokens\".format(sum(frequency.values())))\n",
        "print(\"word types = {} words\".format(len(frequency.keys())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokens = 115172 tokens\n",
            "word types = 11831 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiUa1Q9VyoZV"
      },
      "source": [
        "# 2. Unigram, Bigram, and Trigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA3DqGkQyuWY"
      },
      "source": [
        "This section will focus on generating N-grams. \n",
        "First, we define a function to return n-grams' frequencies given word tokens and an integer ```n```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rrIRwlezZLA"
      },
      "source": [
        "def N_gram(tokens, n):\n",
        "  frequency = defaultdict(lambda: 0)\n",
        "  for i in range(len(tokens)-n+1):\n",
        "    n_gram = tuple(tokens[i:i+n])\n",
        "    frequency[n_gram] += 1\n",
        "  return frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uZNUI8stWtI"
      },
      "source": [
        "Then, we generate unigram, bigram, and trigram and examine 5 most frequent elements in each of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP0Q4u823aMx"
      },
      "source": [
        "unigram = N_gram(tokens, 1)\n",
        "bigram = N_gram(tokens, 2)\n",
        "trigram = N_gram(tokens, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBrtnHK54kwN",
        "outputId": "432fd77e-c314-40dc-df7c-3ff5ec510bef"
      },
      "source": [
        "import operator\n",
        "print(\"#\"*20, \"UNIGRAM\", \"#\"*20)\n",
        "for n_gram, frequency in sorted(unigram.items(), key=operator.itemgetter(1), reverse=True)[:5]: print(n_gram, frequency)\n",
        "\n",
        "print(\"#\"*20, \"BIGRAM\", \"#\"*20)\n",
        "for n_gram, frequency in sorted(bigram.items(), key=operator.itemgetter(1), reverse=True)[:5]: print(n_gram, frequency)\n",
        "\n",
        "print(\"#\"*20, \"TRIGRAM\", \"#\"*20)\n",
        "for n_gram, frequency in sorted(trigram.items(), key=operator.itemgetter(1), reverse=True)[:5]: print(n_gram, frequency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#################### UNIGRAM ####################\n",
            "(',',) 6504\n",
            "('the',) 5785\n",
            "('of',) 3458\n",
            "('a',) 2439\n",
            "('was',) 2299\n",
            "#################### BIGRAM ####################\n",
            "('of', 'the') 769\n",
            "(',', 'and') 762\n",
            "('in', 'the') 525\n",
            "(',', 'the') 324\n",
            "('it', 'was') 318\n",
            "#################### TRIGRAM ####################\n",
            "(',', \"'\", 'said') 105\n",
            "(',', 'and', 'the') 95\n",
            "(',', \"'\", 'he') 95\n",
            "(\"'\", 'he', 'said') 72\n",
            "('a', 'sort', 'of') 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDs0tHFKtxs2"
      },
      "source": [
        "Since we consider punctuations as tokens, it is unsurprised that at least one of them appears in among the most frequent unigrams. In fact, semicolon (,) appears instead of period (.). Therefore, we can probably surmise that most sentences in the input file are quite complex, comprising of at least two independent clauses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHPPM9dShv3j"
      },
      "source": [
        "# 3. Export Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ykmq6Qwf3Kc"
      },
      "source": [
        "Now, we export the ipython notebook as a pdf file by using python script available on [GitHub](https://github.com/brpy/colab-pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y15FD66ff9g9",
        "outputId": "4a44ffe7-17c5-4f8e-faae-94391efbac6b"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-15 05:59:50--  https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864 (1.8K) [text/plain]\n",
            "Saving to: ‘colab_pdf.py’\n",
            "\n",
            "colab_pdf.py        100%[===================>]   1.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-15 05:59:50 (35.3 MB/s) - ‘colab_pdf.py’ saved [1864/1864]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "3eNb6D5fgINe",
        "outputId": "1fd9bb7a-0103-46f8-a4ea-e66899008d9b"
      },
      "source": [
        "from colab_pdf import colab_pdf\n",
        "colab_pdf('Assignment-week4.ipynb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Extracting templates from packages: 100%\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/Assignment-week4.ipynb to pdf\n",
            "[NbConvertApp] Writing 33230 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 35301 bytes to /content/drive/My Drive/Assignment-week4.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ed7669d4-6981-4e20-998e-51f426ec7baf\", \"Assignment-week4.pdf\", 35301)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File ready to be Downloaded and Saved to Drive'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}